<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Programming | The Blog from Darel]]></title>
  <link href="http://darelf.github.io/blog/categories/programming/atom.xml" rel="self"/>
  <link href="http://darelf.github.io/"/>
  <updated>2014-04-09T13:44:18-04:00</updated>
  <id>http://darelf.github.io/</id>
  <author>
    <name><![CDATA[Darel Finkbeiner]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Evented]]></title>
    <link href="http://darelf.github.io/blog/2014/04/09/evented/"/>
    <updated>2014-04-09T13:06:49-04:00</updated>
    <id>http://darelf.github.io/blog/2014/04/09/evented</id>
    <content type="html"><![CDATA[<p>One of the big advantages of <a href="http://nodejs.org">node.js</a> is that of events and
an emphasis on asynchronous programming.</p>

<!--more-->


<p>It is almost a given, at this point, that if you learned all your programming
from college, you probably didn&rsquo;t learn how to write asynchronous functions. In
fact, you were probably warned away from dealing with things that were asynchronous
directly. Instead using pre-packaged frameworks that exposed asynchronous things
( like I/O ) as if they were synchronous, solving your problem for you.</p>

<p>That&rsquo;s fine. But node.js don&rsquo;t truck with that.</p>

<p>If you are going to do something in node.js, you need to deal with asynchronous
functions. You need to create functions that can be called asynchronously, and that
play well in that kind of ecosystem. Here are a few tips to help you make the
transition from one to the other.</p>

<p>** Problem &ndash; Big File</p>

<p>You have to create many millions of rows of data and output them to a file. You
can&rsquo;t keep the whole file in memory and write it all at once. You may have infinite
time but you don&rsquo;t have infinite memory.</p>

<p>Step one is to have a function that creates a row of data.</p>

<pre><code>function makeOneRow(item) {
  // do something fancy
  return rowOfData
}
</code></pre>

<p>Step two is to create a writable stream. For our purposes, we say a file, so we can
use the <code>fs</code> module.</p>

<pre><code>var fs = require('fs')
var outputFile = fs.createWriteableStream('name-of-output-file.txt')
</code></pre>

<p>Step three is to loop through your list of items to create and write them to the file</p>

<pre><code>listOfItems.forEach(function (item) {
  outputFile.write(makeOneRow(item))
})
outputFile.end()
</code></pre>

<p>&ldquo;Wait a second!&rdquo; I hear you exclaim. &ldquo;What if the makeOneRow() is also asynchronous?&rdquo;
Maybe the call that makes a row has to call the database, which is almost certainly an
asynchronous call. In this case, we would quickly crash.</p>

<p>One solution to this problem is my favorite: events. The stream functionality that is
available in node.js makes events a natural choice for this. Let&rsquo;s say for the moment
that creating a row takes a database call that uses a callback.</p>

<pre><code>function databaseCall(item, callback) {
  // do database stuff and store results in processData variable.
  callback(processData)
}
</code></pre>

<p>What do we do with this callback? Well, if we are using events we need a javascript
object that is an <code>EventEmitter</code>. So let&rsquo;s create one real quick.</p>

<pre><code>var util = require('util')
var events = require('events')

function MyObject() {
  events.EventEmitter.call(this)
}
util.inherits(MyObject, events.EventEmitter)

var myobject = new MyObject()
</code></pre>

<p>Ok. What does this give us? Well, it gives us an object that can call <code>emit</code> in order
to send out events to listeners. So let&rsquo;s adapt our function that makes items to use it.</p>

<pre><code>MyObject.prototype.makeOneRow = function(item) {
  var self = this
  databaseCall(item, function(processData) {
    self.emit('data', processData)
  })
}
</code></pre>

<p>Now all we have to do is listen for data events and write them to the file. The other
thing is calling only one of these at a time, since we care about the order that
things get written to the file. So when listening for the &lsquo;data&rsquo; event, check if
you have more items, and if you do then ask for another one.</p>

<pre><code>function doOne() {
  var nextItem = listOfItems.splice(0,1)
  myobject.makeOneRow(nextItem)
}

myobject.on('data', function(newdata) {
  outputFile.write(newdata)
  if (listOfItems.length &gt; 0) doOne()
})
</code></pre>

<p>Even better would be to allow the object to keep track of how many items, and send
an &lsquo;end&rsquo; event when it reached the end. I&rsquo;ll leave that as an exercise. (You gotta
learn to do it yourself eventually, right?)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simple Programming]]></title>
    <link href="http://darelf.github.io/blog/2014/04/02/simple-programming/"/>
    <updated>2014-04-02T08:19:57-04:00</updated>
    <id>http://darelf.github.io/blog/2014/04/02/simple-programming</id>
    <content type="html"><![CDATA[<p>&ldquo;All that is complex is not useful. All that is useful is simple.&rdquo; &mdash; Mikhail Kalashnikov</p>

<p>&ldquo;A scientific theory should be as simple as possible, but no simpler.&rdquo; &mdash; Albert Einstein</p>

<p>Ran into this problem recently. It&rsquo;s very difficult to deal with programmers sometimes,
as they tend to very often be right in the face of naysayers, they get used to it and
just assume that their way is right even when it&rsquo;s obviously wrong.</p>

<!--more-->


<p>Programmers fall for the same failings of the rest of humanity, such as resisting change,
or taking perverse pride in something they did even when that thing is awful. I hear so
many times from people that they &ldquo;worked so hard&rdquo; on something. And I think, &ldquo;But that
something you worked hard on is horrible, you should not be proud of it or resist replacing
it with something good.&rdquo; (I have rather lengthy internal monologues sometimes)</p>

<p>The big issues when code gets complex center around maintainability. That extends both to
bringing in new programmers on a project, as well as revisiting code that you have written
in the past. It also affects interoperability. Simple interfaces are better than complex
ones. As simple as possible while still accomplishing all the goals.</p>

<p>So, if you are familiar with one tool and you start using it to solve a problem that it
is not designed for, you will inexorably end up with a much more complex system than
necessary. If you use techniques that aren&rsquo;t tailored to the task at hand, you will
have the same problem.</p>

<p>Choices in programming should be made by programmers not by business decisions. Goals
and products should be chosen by business not by programmers. Once you start mixing, you
end up with a horrible mess that is far more costly and a nightmare to maintain. When
business makes the decision on programming language, techniques, etc. you are headed into
the dystopian realm of ill-functioning, difficult to maintain software.</p>

<p>&ldquo;But it&rsquo;s easier to hire Java (or insert other horrible languages) programmers&hellip;&rdquo;
I hear you cry. First, I call BS on that on its face. Second, you don&rsquo;t want programmers
that are easy to hire. That&rsquo;s like saying it&rsquo;s easier to hire unskilled laborers.
Of course it is. There&rsquo;s a reason for that.</p>

<p>Advice is only as good as the people taking it.</p>

<p>When we say &ldquo;make it simple&rdquo;, we mean also &ldquo;make it work and work well&rdquo;. Don&rsquo;t over-simplify,
but choose the right tool and the right techniques and make it as simple as it can be.
Make code easy to reason about. Make it easy to extend. Make it easy to integrate. Make it
easy. Simple is easy. Complex is hard.</p>

<p>Just because you haven&rsquo;t encountered a technique before doesn&rsquo;t mean it&rsquo;s hard. If you
look at something like <a href="http://en.wikipedia.org/wiki/Bitmap_index">WAH</a> or CONCISE it may look
difficult and complex at first, but it&rsquo;s really rather simple. Honestly, when you realize
that half the compression is simply packing and run length encoding it&rsquo;s very easy to
grasp, and importantly easy to reason about.</p>

<p>Keep code simple. Keep inter-process interfaces simple. Keep APIs simple. Make the so-called
surface area easy to grasp, easy to reason about. Keep it easy to maintain, easy to upgrade,
easy to extend. You can&rsquo;t see all futures, but if you make the code simple it will be
easier to adapt to new and changing environments.</p>

<p>If you don&rsquo;t, you are inviting trouble for yourself and others. You are the Causer of
Problems. Don&rsquo;t be the bad guy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Synchonization]]></title>
    <link href="http://darelf.github.io/blog/2014/03/18/data-synchonization/"/>
    <updated>2014-03-18T09:40:31-04:00</updated>
    <id>http://darelf.github.io/blog/2014/03/18/data-synchonization</id>
    <content type="html"><![CDATA[<p>Data. Always getting out of sync. You&rsquo;d think it would learn. You&rsquo;d think that
maybe if you tightened up your code it would never get out of sync.</p>

<p>Well. Data don&rsquo;t care what you think. That&rsquo;s why CRC, etc.</p>

<!--more-->


<p>At a high level, though, it&rsquo;s very easy to see why data needs to be synchronized
and how it can get messed up. Two people updating a spreadsheet separately can
easily end up with two different versions, and then needing to synchronize them
(aka merge).</p>

<p>What are some of our options? Glad you asked.</p>

<p>One of my favorite ways of making it work is &ldquo;append only&rdquo;. That is, you can never
delete anything, you can only add to it. This makes synchronization easy. You just
ask if they have any items you don&rsquo;t have, and that&rsquo;s it. Synchronized.</p>

<p>Updates are also fairly easy. Each record having a time it was last updated, or
a sequence number or whatever, and you can just ask for the latest versions. Again,
that&rsquo;s pretty much a snap.</p>

<p>What about deletes?</p>

<p>Ah. Well, this is only slightly more complicated. In this case, what you really need
is a &ldquo;history&rdquo;. This history can actually just be items that were deleted. The
simplest design, of course, is to have each record have a sync field (either a
sequence or a timestamp) and also have a &ldquo;tombstone&rdquo; or list of items that were
deleted. You don&rsquo;t even need to know when, necessarily, if a given id can never
be reused.</p>

<p>If they can be reused, then you need to also include the sync field in the history,
that way something might be &ldquo;brought back to life&rdquo; later. This kind of necromancy
is not uncommon in many systems.</p>

<p>So, what do we have? Well, in order to keep multiple copies of the same data in
sync with each other efficiently (that is, not just checking every line and
replacing the whole dataset), a system should have: a history that lists the latest
version of each item with a synchronization field of some kind, such as a
monotonic timestamp, and the action of &lsquo;created&rsquo;, &lsquo;updated&rsquo;, or &lsquo;deleted&rsquo;.</p>

<p>One of the issues is with timestamps. If you need to synchronize in real-time
between multiple nodes that share data, and you have a high rate of operations,
your timestamps will probably cause you issues. The issues are directly related
to the same problem of keeping two clocks in sync. When you have multiple
processes on different machines doing multiple updates per second each, you are
going to have to come up with some scheme for ordering them.</p>

<p>This can be important for updates/deletes, depending on how you structure things.
Very often, the solutions end up being a combination of timestamp, process name,
and a sequence number. There are many different approaches, but they all follow
the same pattern. A decent approach, if you are satisfied with &ldquo;rough&rdquo; ordering,
would be something like <a href="https://github.com/twitter/snowflake">Twitter Snowflake</a>.</p>

<p>But don&rsquo;t stop there. There&rsquo;s a lot to learn about this topic, and it is
fascinating in and of itself.</p>

<p>Warning: make sure you don&rsquo;t over-engineer your solution. If you are only worried
about synchronizing data that gets updated hourly or daily, then don&rsquo;t overthink
it and come up with some really complicated system.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Distributed Jobs]]></title>
    <link href="http://darelf.github.io/blog/2014/03/11/distributed-jobs/"/>
    <updated>2014-03-11T13:12:33-04:00</updated>
    <id>http://darelf.github.io/blog/2014/03/11/distributed-jobs</id>
    <content type="html"><![CDATA[<p>This post is supposed to be about network queuing of jobs that can be
distributed into discrete tasks which themselves can be completed in
any order. And about the boiled down <a href="http://github.com/darelf/stream-queue">stream-queue</a>
npm package that I have published. I will diverge just a bit here to
talk a little bit of background.</p>

<!--more-->


<p>STREAMS. Just think about that. Streams. Think of all the things it implies
in programming to talk about streams and streaming.</p>

<p>Probably number one is networking, that is TCP (or whatever) network
sockets that are read/write and have very well-established, at this point,
semantics. Open, close, pause, end, read, write, etc. Depending on the
exact stream, of course, things may be slightly different, and we&rsquo;ve seen
quite a lot of semantics built on top of streams.</p>

<p>Nodejs, of course, does networking really well. In fact, that may be the
number one purpose for most programmers using it. It&rsquo;s just really
fantastic at networking. Your code can worry about what goes on just
above the socket level, with the streams. They also give you an
abstraction, so that there are file streams, etc. helping you out.</p>

<p>This is all fairly normal in programming, and streams have been around
for a long time. UNIX pipes are probably the most common stream-semantic
thing that programmers use, but they are everywhere in just about every
language that&rsquo;s of any use.</p>

<p>So&hellip;</p>

<p>Back to jobs. If you have a big job that can broken down into discreet
tasks such that those tasks can happen in any order, you have something
that can be distributed. And by distributed, I mean among separate processes,
each process accomplishing one of the tasks towards the goal of completing
the job.</p>

<p>What <code>stream-queue</code> is designed to do is give the basic functionality of
a distributed queue that I needed. I need to have it manage the list of
tasks, how many total tasks there were at the beginning, how many have
been completed and how many are left to do. It needs to notify me of
when it starts, when it ends, and each time a step is taken on the way
(progress).</p>

<p>These are all straightforward. They need to not depend on time. That is,
having a unique timestamp should not be required, seeing as how there
might be many workers concurrently processing tasks, and some will inevitably
happen at the same timestamp.</p>

<p>The workers should be able to spin up or shutdown without consequence to
the working of the queue as it is in progress. One future upgrade to the
<code>stream-queue</code> package will be the ability to re-do a task if a worker
gets shutdown before completing the task. Also, having a standard semantic
for asking the queue to shutdown one of its workers, either simply to
reduce the load, or possibly even by name.</p>

<p>Others might be the ability to manage multiple kinds of workers, and
have those workers advertise the kind of work they can do and let the
queue manager distribute jobs accordingly.</p>

<p>So that&rsquo;s where it&rsquo;s at. A lot of interesting tweaks to be made, and a
lot of progress towards better features.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wrap Those Web Sockets]]></title>
    <link href="http://darelf.github.io/blog/2014/03/04/wrap-those-web-sockets/"/>
    <updated>2014-03-04T10:59:10-05:00</updated>
    <id>http://darelf.github.io/blog/2014/03/04/wrap-those-web-sockets</id>
    <content type="html"><![CDATA[<p>Here&rsquo;s the situation: You have a websocket-friendly routing infrastructure that
only has a single port available on each machine. (at least that you can use) But
you have some awesome client-server stuff that runs completely in the background
and is not really related to browsers or anything. You currently, and brilliantly,
have them speaking over regular node streams using the <code>net</code> functionality.</p>

<p>What to do?</p>

<!--more-->


<p>The routing structure only routes http requests. You can&rsquo;t just redirect regular
socket connections.</p>

<p>WEBSOCKETS TO THE RESCUE!  Websockets are just sockets, but they are negotiated
over http. Oh yeah they are. So they can be routed by your infrastructure that
supports websockets. (like, say, <a href="http://github.com/substack/bouncy">bouncy</a>, but
there are plenty of other examples).</p>

<p>But you aren&rsquo;t using a browser. There&rsquo;s no browser here. Hmm.</p>

<p>You go check out <code>npm</code> and find out that maxogden has already solved your problem.
That&rsquo;s what you do. He has already wrapped websockets with stream functionality in
a project he calls <a href="http://github.com/maxogden/websocket-stream">websocket-stream</a></p>

<p>Now your carefully designed code that leverages the node stream api can continue
on untouched simply by wrapping the websockets with this little gem.</p>

<p>Enjoy!</p>
]]></content>
  </entry>
  
</feed>
