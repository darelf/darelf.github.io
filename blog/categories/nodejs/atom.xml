<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Nodejs | The Blog from Darel]]></title>
  <link href="http://darelf.github.io/blog/categories/nodejs/atom.xml" rel="self"/>
  <link href="http://darelf.github.io/"/>
  <updated>2014-04-17T15:08:02-04:00</updated>
  <id>http://darelf.github.io/</id>
  <author>
    <name><![CDATA[Darel Finkbeiner]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Evented]]></title>
    <link href="http://darelf.github.io/blog/2014/04/09/evented/"/>
    <updated>2014-04-09T13:06:49-04:00</updated>
    <id>http://darelf.github.io/blog/2014/04/09/evented</id>
    <content type="html"><![CDATA[<p>One of the big advantages of <a href="http://nodejs.org">node.js</a> is that of events and
an emphasis on asynchronous programming.</p>

<!--more-->


<p>It is almost a given, at this point, that if you learned all your programming
from college, you probably didn&rsquo;t learn how to write asynchronous functions. In
fact, you were probably warned away from dealing with things that were asynchronous
directly. Instead using pre-packaged frameworks that exposed asynchronous things
( like I/O ) as if they were synchronous, solving your problem for you.</p>

<p>That&rsquo;s fine. But node.js don&rsquo;t truck with that.</p>

<p>If you are going to do something in node.js, you need to deal with asynchronous
functions. You need to create functions that can be called asynchronously, and that
play well in that kind of ecosystem. Here are a few tips to help you make the
transition from one to the other.</p>

<p>** Problem &ndash; Big File</p>

<p>You have to create many millions of rows of data and output them to a file. You
can&rsquo;t keep the whole file in memory and write it all at once. You may have infinite
time but you don&rsquo;t have infinite memory.</p>

<p>Step one is to have a function that creates a row of data.</p>

<pre><code>function makeOneRow(item) {
  // do something fancy
  return rowOfData
}
</code></pre>

<p>Step two is to create a writable stream. For our purposes, we say a file, so we can
use the <code>fs</code> module.</p>

<pre><code>var fs = require('fs')
var outputFile = fs.createWriteableStream('name-of-output-file.txt')
</code></pre>

<p>Step three is to loop through your list of items to create and write them to the file</p>

<pre><code>listOfItems.forEach(function (item) {
  outputFile.write(makeOneRow(item))
})
outputFile.end()
</code></pre>

<p>&ldquo;Wait a second!&rdquo; I hear you exclaim. &ldquo;What if the makeOneRow() is also asynchronous?&rdquo;
Maybe the call that makes a row has to call the database, which is almost certainly an
asynchronous call. In this case, we would quickly crash.</p>

<p>One solution to this problem is my favorite: events. The stream functionality that is
available in node.js makes events a natural choice for this. Let&rsquo;s say for the moment
that creating a row takes a database call that uses a callback.</p>

<pre><code>function databaseCall(item, callback) {
  // do database stuff and store results in processData variable.
  callback(processData)
}
</code></pre>

<p>What do we do with this callback? Well, if we are using events we need a javascript
object that is an <code>EventEmitter</code>. So let&rsquo;s create one real quick.</p>

<pre><code>var util = require('util')
var events = require('events')

function MyObject() {
  events.EventEmitter.call(this)
}
util.inherits(MyObject, events.EventEmitter)

var myobject = new MyObject()
</code></pre>

<p>Ok. What does this give us? Well, it gives us an object that can call <code>emit</code> in order
to send out events to listeners. So let&rsquo;s adapt our function that makes items to use it.</p>

<pre><code>MyObject.prototype.makeOneRow = function(item) {
  var self = this
  databaseCall(item, function(processData) {
    self.emit('data', processData)
  })
}
</code></pre>

<p>Now all we have to do is listen for data events and write them to the file. The other
thing is calling only one of these at a time, since we care about the order that
things get written to the file. So when listening for the &lsquo;data&rsquo; event, check if
you have more items, and if you do then ask for another one.</p>

<pre><code>function doOne() {
  var nextItem = listOfItems.splice(0,1)
  myobject.makeOneRow(nextItem)
}

myobject.on('data', function(newdata) {
  outputFile.write(newdata)
  if (listOfItems.length &gt; 0) doOne()
})
</code></pre>

<p>Even better would be to allow the object to keep track of how many items, and send
an &lsquo;end&rsquo; event when it reached the end. I&rsquo;ll leave that as an exercise. (You gotta
learn to do it yourself eventually, right?)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Distributed Jobs]]></title>
    <link href="http://darelf.github.io/blog/2014/03/11/distributed-jobs/"/>
    <updated>2014-03-11T13:12:33-04:00</updated>
    <id>http://darelf.github.io/blog/2014/03/11/distributed-jobs</id>
    <content type="html"><![CDATA[<p>This post is supposed to be about network queuing of jobs that can be
distributed into discrete tasks which themselves can be completed in
any order. And about the boiled down <a href="http://github.com/darelf/stream-queue">stream-queue</a>
npm package that I have published. I will diverge just a bit here to
talk a little bit of background.</p>

<!--more-->


<p>STREAMS. Just think about that. Streams. Think of all the things it implies
in programming to talk about streams and streaming.</p>

<p>Probably number one is networking, that is TCP (or whatever) network
sockets that are read/write and have very well-established, at this point,
semantics. Open, close, pause, end, read, write, etc. Depending on the
exact stream, of course, things may be slightly different, and we&rsquo;ve seen
quite a lot of semantics built on top of streams.</p>

<p>Nodejs, of course, does networking really well. In fact, that may be the
number one purpose for most programmers using it. It&rsquo;s just really
fantastic at networking. Your code can worry about what goes on just
above the socket level, with the streams. They also give you an
abstraction, so that there are file streams, etc. helping you out.</p>

<p>This is all fairly normal in programming, and streams have been around
for a long time. UNIX pipes are probably the most common stream-semantic
thing that programmers use, but they are everywhere in just about every
language that&rsquo;s of any use.</p>

<p>So&hellip;</p>

<p>Back to jobs. If you have a big job that can broken down into discreet
tasks such that those tasks can happen in any order, you have something
that can be distributed. And by distributed, I mean among separate processes,
each process accomplishing one of the tasks towards the goal of completing
the job.</p>

<p>What <code>stream-queue</code> is designed to do is give the basic functionality of
a distributed queue that I needed. I need to have it manage the list of
tasks, how many total tasks there were at the beginning, how many have
been completed and how many are left to do. It needs to notify me of
when it starts, when it ends, and each time a step is taken on the way
(progress).</p>

<p>These are all straightforward. They need to not depend on time. That is,
having a unique timestamp should not be required, seeing as how there
might be many workers concurrently processing tasks, and some will inevitably
happen at the same timestamp.</p>

<p>The workers should be able to spin up or shutdown without consequence to
the working of the queue as it is in progress. One future upgrade to the
<code>stream-queue</code> package will be the ability to re-do a task if a worker
gets shutdown before completing the task. Also, having a standard semantic
for asking the queue to shutdown one of its workers, either simply to
reduce the load, or possibly even by name.</p>

<p>Others might be the ability to manage multiple kinds of workers, and
have those workers advertise the kind of work they can do and let the
queue manager distribute jobs accordingly.</p>

<p>So that&rsquo;s where it&rsquo;s at. A lot of interesting tweaks to be made, and a
lot of progress towards better features.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wrap Those Web Sockets]]></title>
    <link href="http://darelf.github.io/blog/2014/03/04/wrap-those-web-sockets/"/>
    <updated>2014-03-04T10:59:10-05:00</updated>
    <id>http://darelf.github.io/blog/2014/03/04/wrap-those-web-sockets</id>
    <content type="html"><![CDATA[<p>Here&rsquo;s the situation: You have a websocket-friendly routing infrastructure that
only has a single port available on each machine. (at least that you can use) But
you have some awesome client-server stuff that runs completely in the background
and is not really related to browsers or anything. You currently, and brilliantly,
have them speaking over regular node streams using the <code>net</code> functionality.</p>

<p>What to do?</p>

<!--more-->


<p>The routing structure only routes http requests. You can&rsquo;t just redirect regular
socket connections.</p>

<p>WEBSOCKETS TO THE RESCUE!  Websockets are just sockets, but they are negotiated
over http. Oh yeah they are. So they can be routed by your infrastructure that
supports websockets. (like, say, <a href="http://github.com/substack/bouncy">bouncy</a>, but
there are plenty of other examples).</p>

<p>But you aren&rsquo;t using a browser. There&rsquo;s no browser here. Hmm.</p>

<p>You go check out <code>npm</code> and find out that maxogden has already solved your problem.
That&rsquo;s what you do. He has already wrapped websockets with stream functionality in
a project he calls <a href="http://github.com/maxogden/websocket-stream">websocket-stream</a></p>

<p>Now your carefully designed code that leverages the node stream api can continue
on untouched simply by wrapping the websockets with this little gem.</p>

<p>Enjoy!</p>
]]></content>
  </entry>
  
</feed>
