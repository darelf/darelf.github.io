<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Nodejs | The Blog from Darel]]></title>
  <link href="http://darelf.github.io/blog/categories/nodejs/atom.xml" rel="self"/>
  <link href="http://darelf.github.io/"/>
  <updated>2014-05-08T15:47:14-04:00</updated>
  <id>http://darelf.github.io/</id>
  <author>
    <name><![CDATA[Darel Finkbeiner]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Bouncing and Routing]]></title>
    <link href="http://darelf.github.io/blog/2014/05/08/bouncing-and-routing/"/>
    <updated>2014-05-08T10:51:08-04:00</updated>
    <id>http://darelf.github.io/blog/2014/05/08/bouncing-and-routing</id>
    <content type="html"><![CDATA[<p>Oh, man. I’ve created two different really great node.js web services/sites and now
I want to host them. Oh man, this is going to be great!</p>

<p>Hmm. I can only expose a single port on a single machine to the outside.
How can I make that work? What if my services are on multiple physical machines?</p>

<h3 id="finding-the-right-server">Finding the Right Server</h3>

<p>Enter <a href="https://github.com/substack/seaport">seaport</a>. This little beauty is great
for registering/discovering web services. It’s soooooo much easier than anything
else you’ve ever tried. (Although, as of this writing, there is an issue that needs
patching, and there’s even a pull request for it at
<a href="https://github.com/substack/seaport/pull/57">https://github.com/substack/seaport/pull/57</a>
so I would just apply that little bit of code if I were you… at least until it gets
merged)</p>

<p>Let me show you:</p>

<pre><code>var seaport = require('seaport')
var ports = seaport.createServer()
ports.listen(9001)
</code></pre>

<p>That’s it for the server.</p>

<p>Now, your node.js webservices need to pick their port by asking that seaport
server we just created. Let’s say Service One is on the same physical machine.</p>

<pre><code>var http = require('http')
var seaport = require('seaport').connect('localhost', 9001, { secret: 'totallysecret' })
var port = seaport.register('serviceone@0.0.1')
http.createServer(function(req,res) { res.end("I'm Service One!\n") }).listen(port)
</code></pre>

<p>What just happened?</p>

<p>Well, we connected to the registry, and then we said “When someone asks for ‘serviceone’ with
a version of ‘0.0.1’, then you can direct them our way.” The seaport then gives you a port number
and you use that in your server.</p>

<p>Let’s now say that Service Two is on a different physical machine. The seaport is on a server
named “kahleesi” while Service Two is running on “katniss”. So first connect to the seaport
server:</p>

<pre><code>... stuff ...
var seaport = require('seaport').connect('kahleesi', 9001, {secret: 'totallysecret'})
var port = seaport.register('servicetwo@0.0.1')
... actual server ...
</code></pre>

<p>Now that we have the services all talking to each other… what now?</p>

<h2 id="url-routing">URL Routing</h2>

<p>Let’s say your web services only use straight up, request/response, HTTP. No
websockets. And you aren’t allowed to have multiple hostnames for some reason.</p>

<p>You might be tempted to use <a href="https://github.com/substack/bouncy">bouncy</a> and under other
circumstances, I would agree. If you can use vhost style routing, then bouncy is the
perfect solution to your problem.</p>

<p>The problem here is that without hostname differentiation,
the client browser will only get the socket once. This means routing by URL won’t work.
Or at least, it will seem to work until you try to open another tab and go to a different
URL on that same host… then you’ll realize that bouncy is connecting the socket directly
and once it does that it doesn’t look at it ever again so it has no idea you wanted something
different. Kind of a nightmare.</p>

<p>So… we build a proxy. In other languages, let’s say “lesser” languages, building such
a proxy would be hard work.  But in node.js it is really easy.  Here’s how I did it.</p>

<p>First we will create a combination seaport/http proxy server so that it can all be
combined into a single process.</p>

<pre><code>var http = require('http')
var seaport = require('seaport')
var argv = require('optimist').default('p', 8080).argv

var ports = seaport.createServer()
ports.listen(9001)

var server = http.createServer(function(req,res) {
  if (/^\/serviceone/.test(req.url)) {
    var service = 'serviceone@0.0.1'
  } else if (/^\/servicetwo/.test(req.url)) {
    var service = 'servicetwo@0.0.1'
  }
  
  if (service) {
    // Here's where we ask seaport if it knows about this service
    var ps = ports.query(service)
    if (ps.length === 0) {
      res.end("No Service Available")
    } else {
      // At this point, ps is an array of host,port that can respond to these requests
      // I'm just going to take the first one...
      
      // Here's the actual proxy stuff
      var opts = {host: ps[0].host, port: ps[0].port, method: req.method,
                  path: req.url, headers: req.headers}
      var proxy = http.request(opts)
      proxy.on('response', function (response) {
        response.on('data', function(chunk) {
          res.write(chunk)
        })
        response.on('end', function() { res.end() })
        res.writeHead(response.statusCode, response.headers)
      })
      req.on('data', function(chunk) { proxy.write(chunk) })
      req.on('end', function() { proxy.end() })
    }        
  } else {
    // Well, we have no idea what they want
    res.end("Whatever, dude.")
  }
}).listen(argv.p)
</code></pre>

<p>I don’t know if you’ve ever tried to write an http proxy by hand before, but in most
other langauges it takes a lot more work than this.</p>

<p>Basically, we are taking their request, looking for where the “real” server is, and
then creating a new request with the host and port of the “real” server and then
returning the response back to the client.  NONE THE WISER.</p>

<p>We just hook into the <code>data</code> and <code>end</code> events and just rewrite stuff. It’s pretty
straight forward.</p>

<p>This way it doesn’t really matter how many services, or even if you have different
versions, or whatever. You can just route to your heart’s content based on any
criteria you want. You have the full request and headers to play with, so you can
make up whatever rules you can code in javascript.</p>

<p>Have Fun!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using NPM Like a Boss]]></title>
    <link href="http://darelf.github.io/blog/2014/04/22/using-npm-like-a-boss/"/>
    <updated>2014-04-22T13:50:43-04:00</updated>
    <id>http://darelf.github.io/blog/2014/04/22/using-npm-like-a-boss</id>
    <content type="html"><![CDATA[<p>So I have to keep track of what I work on each week and send
out an email with a short description of each. Yeah. You know.</p>

<p>I thought “What a great idea for making a script that handles all
that with a simple command line!” I hear you mumbling “nerd”. That’s
OK. I can take it.
<!--more-->
## Enter… The Script</p>

<p>Well, I just need a list, each list can live in a text file that I
can then email to my boss each week. What do I need?</p>

<p>Very little. I use <a href="http://momentjs.com">moment</a> to figure out what
week I’m working on automatically and store it in the correct file.
I also use <a href="http://github.com/substack/minimist#minimist">minimist</a> to do
command line parsing since I barely need anything there.</p>

<p>So the beginning looked like this:</p>

<pre><code>var fs = require('fs')
var moment = require('moment')
</code></pre>

<p>Then I realized I (or someone) might want it daily, so I add a
default. And while I’m at it, let’s define a working directory
for the files.</p>

<pre><code>var defaults = { weekly: true, workdir: process.env.HOME + '/worklist/' }
var argv = require('minimist')(process.argv.slice(2), {default: defaults})
</code></pre>

<p>You could override those, for instance with <code>--workdir=/my/crazy/directory/</code> but
hopefully these are sane defaults.</p>

<p>Now let’s figure out what file we should be working with:</p>

<pre><code>var m = moment()
if (argv.weekly) {
  m = moment().startOf('week').add('days', 1)
}

var fname = argv.workdir + m.format('YYYY-MM-DD') + '.txt'
</code></pre>

<p>The file name will now have the date, or Monday’s date when we are doing weekly
files. This let’s us choose.</p>

<p>Next we read in the file, if it exists, and parse the contents:</p>

<pre><code>var contents = []
fs.readFile(fname, function(err, data) {
  if (!err) {
    contents = data.toString().split('\n').filter(Boolean)
  }
  parseCommands()
})
</code></pre>

<p>That <code>parseCommands()</code> function will need to be defined. It’s basically the
entry point for looking at any command the user sent and taking the correct
action. I decided I only needed 3 commands:  list, add, and delete.</p>

<pre><code>function parseCommands() {
  if (argv._.indexOf('list') &gt; -1) {
    contents.forEach(function(v,i) { console.log("%d: %s", i, v) })
  }
  if (argv.add) {
    contents.push(argv.add)
    saveContents()
  }
  if (argv.d) {
    if (contents.length &gt; argv.d) {
      delete contents[argv.d]
      saveContents()
    }
  }
}
</code></pre>

<p>What we have going on here is a <code>list</code> command that just dumps the contents.
Next, we have a <code>--add="This is the item to add"</code> command that will add
a single line to our file, and then it will save the file. (see below for that)
And finally the <code>-d #</code> command that takes the number of the item to delete.</p>

<p>Not really pretty, but since I’m mostly just adding and listing, it seems
to work just fine. And since the files are just text files, I can always
edit them by hand, feed them into other programs or utilities easily, and
generally use any of my typical Unixy workflows.</p>

<p>Oh, and saving the file just looks like this:</p>

<pre><code>function saveContents() {
  fs.writeFile(fname, contents.join('\n') + '\n')
}
</code></pre>

<h2 id="enter-npm">Enter… NPM!!!</h2>

<p>How does the <code>npm</code> utility enter in to this? I’m glad you asked!</p>

<p>NPM has a facility called “bin” in the <code>package.json</code> file that allows you
to define so-called <code>binary</code> programs to be installed. In my case, this is
just a node js script as shown above.</p>

<p>What steps do I take to make this work as a command? I.e. <code>worklist list</code> or
<code>worklist --add="I totally did important work just now"</code></p>

<h3 id="step-one">Step One</h3>

<p>Add the “shebang” at the beginning of the script telling it to use node.</p>

<pre><code>#!/usr/bin/env node
</code></pre>

<p>For the script, that’s it.</p>

<h3 id="step-two">Step Two</h3>

<p>Add the entry into <code>package.json</code> telling it that the script is a binary.
For our example, I put the script in <code>./bin/worklist.js</code> so that’s what I
will tell NPM. Also, I need to tell it what I want the command to look like,
which in this example is <code>worklist</code> so that I can just type <code>worklist</code> on the
command line and it will run my script in node automatically.</p>

<pre><code>{
  ... bunch of normal package.json stuff ...
  "bin": {
    "worklist": "./bin/worklist.js"
  }
}
</code></pre>

<h3 id="step-three">Step Three</h3>

<p>Install it globally.</p>

<pre><code>npm install -g
</code></pre>

<p>You will notice NPM telling you that it is linking the script into your path
somewhere. I believe on Windows that this also automatically wraps it in a
<code>.cmd</code> for you. WIN.</p>

<h3 id="step-four">Step Four</h3>

<p>Pour a hot cup of coffee, light up your favorite cigar, and enjoy. THIS STEP
IS NOT OPTIONAL.</p>

<h2 id="feedback">Feedback</h2>

<p>Please, please, please, send me feedback, ask me questions. Yeah. You can do it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Evented]]></title>
    <link href="http://darelf.github.io/blog/2014/04/09/evented/"/>
    <updated>2014-04-09T13:06:49-04:00</updated>
    <id>http://darelf.github.io/blog/2014/04/09/evented</id>
    <content type="html"><![CDATA[<p>One of the big advantages of <a href="http://nodejs.org">node.js</a> is that of events and
an emphasis on asynchronous programming.
<!--more-->
It is almost a given, at this point, that if you learned all your programming
from college, you probably didn’t learn how to write asynchronous functions. In
fact, you were probably warned away from dealing with things that were asynchronous
directly. Instead using pre-packaged frameworks that exposed asynchronous things
( like I/O ) as if they were synchronous, solving your problem for you.</p>

<p>That’s fine. But node.js don’t truck with that.</p>

<p>If you are going to do something in node.js, you need to deal with asynchronous
functions. You need to create functions that can be called asynchronously, and that
play well in that kind of ecosystem. Here are a few tips to help you make the
transition from one to the other.</p>

<p>** Problem - Big File</p>

<p>You have to create many millions of rows of data and output them to a file. You
can’t keep the whole file in memory and write it all at once. You may have infinite
time but you don’t have infinite memory.</p>

<p>Step one is to have a function that creates a row of data.
    function makeOneRow(item) {
      // do something fancy
      return rowOfData
    }</p>

<p>Step two is to create a writable stream. For our purposes, we say a file, so we can
use the <code>fs</code> module.
    var fs = require(‘fs’)
    var outputFile = fs.createWriteableStream(‘name-of-output-file.txt’)</p>

<p>Step three is to loop through your list of items to create and write them to the file
    listOfItems.forEach(function (item) {
      outputFile.write(makeOneRow(item))
    })
    outputFile.end()</p>

<p>“Wait a second!” I hear you exclaim. “What if the makeOneRow() is also asynchronous?”
Maybe the call that makes a row has to call the database, which is almost certainly an
asynchronous call. In this case, we would quickly crash.</p>

<p>One solution to this problem is my favorite: events. The stream functionality that is
available in node.js makes events a natural choice for this. Let’s say for the moment
that creating a row takes a database call that uses a callback.</p>

<pre><code>function databaseCall(item, callback) {
  // do database stuff and store results in processData variable.
  callback(processData)
}
</code></pre>

<p>What do we do with this callback? Well, if we are using events we need a javascript
object that is an <code>EventEmitter</code>. So let’s create one real quick.</p>

<pre><code>var util = require('util')
var events = require('events')

function MyObject() {
  events.EventEmitter.call(this)
}
util.inherits(MyObject, events.EventEmitter)

var myobject = new MyObject()
</code></pre>

<p>Ok. What does this give us? Well, it gives us an object that can call <code>emit</code> in order
to send out events to listeners. So let’s adapt our function that makes items to use it.</p>

<pre><code>MyObject.prototype.makeOneRow = function(item) {
  var self = this
  databaseCall(item, function(processData) {
    self.emit('data', processData)
  })
}
</code></pre>

<p>Now all we have to do is listen for data events and write them to the file. The other
thing is calling only one of these at a time, since we care about the order that
things get written to the file. So when listening for the ‘data’ event, check if
you have more items, and if you do then ask for another one.</p>

<pre><code>function doOne() {
  var nextItem = listOfItems.splice(0,1)
  myobject.makeOneRow(nextItem)
}

myobject.on('data', function(newdata) {
  outputFile.write(newdata)
  if (listOfItems.length &gt; 0) doOne()
})
</code></pre>

<p>Even better would be to allow the object to keep track of how many items, and send
an ‘end’ event when it reached the end. I’ll leave that as an exercise. (You gotta
learn to do it yourself eventually, right?)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Distributed Jobs]]></title>
    <link href="http://darelf.github.io/blog/2014/03/11/distributed-jobs/"/>
    <updated>2014-03-11T13:12:33-04:00</updated>
    <id>http://darelf.github.io/blog/2014/03/11/distributed-jobs</id>
    <content type="html"><![CDATA[<p>This post is supposed to be about network queuing of jobs that can be
distributed into discrete tasks which themselves can be completed in
any order. And about the boiled down <a href="http://github.com/darelf/stream-queue">stream-queue</a>
npm package that I have published. I will diverge just a bit here to
talk a little bit of background.
<!--more-->
STREAMS. Just think about that. Streams. Think of all the things it implies
in programming to talk about streams and streaming.</p>

<p>Probably number one is networking, that is TCP (or whatever) network
sockets that are read/write and have very well-established, at this point,
semantics. Open, close, pause, end, read, write, etc. Depending on the
exact stream, of course, things may be slightly different, and we’ve seen
quite a lot of semantics built on top of streams.</p>

<p>Nodejs, of course, does networking really well. In fact, that may be the
number one purpose for most programmers using it. It’s just really
fantastic at networking. Your code can worry about what goes on just
above the socket level, with the streams. They also give you an
abstraction, so that there are file streams, etc. helping you out.</p>

<p>This is all fairly normal in programming, and streams have been around
for a long time. UNIX pipes are probably the most common stream-semantic
thing that programmers use, but they are everywhere in just about every
language that’s of any use.</p>

<p>So…</p>

<p>Back to jobs. If you have a big job that can broken down into discreet
tasks such that those tasks can happen in any order, you have something
that can be distributed. And by distributed, I mean among separate processes,
each process accomplishing one of the tasks towards the goal of completing
the job.</p>

<p>What <code>stream-queue</code> is designed to do is give the basic functionality of
a distributed queue that I needed. I need to have it manage the list of
tasks, how many total tasks there were at the beginning, how many have
been completed and how many are left to do. It needs to notify me of
when it starts, when it ends, and each time a step is taken on the way
(progress).</p>

<p>These are all straightforward. They need to not depend on time. That is,
having a unique timestamp should not be required, seeing as how there
might be many workers concurrently processing tasks, and some will inevitably
happen at the same timestamp.</p>

<p>The workers should be able to spin up or shutdown without consequence to
the working of the queue as it is in progress. One future upgrade to the
<code>stream-queue</code> package will be the ability to re-do a task if a worker
gets shutdown before completing the task. Also, having a standard semantic
for asking the queue to shutdown one of its workers, either simply to
reduce the load, or possibly even by name.</p>

<p>Others might be the ability to manage multiple kinds of workers, and
have those workers advertise the kind of work they can do and let the
queue manager distribute jobs accordingly.</p>

<p>So that’s where it’s at. A lot of interesting tweaks to be made, and a
lot of progress towards better features.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wrap Those Web Sockets]]></title>
    <link href="http://darelf.github.io/blog/2014/03/04/wrap-those-web-sockets/"/>
    <updated>2014-03-04T10:59:10-05:00</updated>
    <id>http://darelf.github.io/blog/2014/03/04/wrap-those-web-sockets</id>
    <content type="html"><![CDATA[<p>Here’s the situation: You have a websocket-friendly routing infrastructure that
only has a single port available on each machine. (at least that you can use) But
you have some awesome client-server stuff that runs completely in the background
and is not really related to browsers or anything. You currently, and brilliantly,
have them speaking over regular node streams using the <code>net</code> functionality.</p>

<p>What to do?
<!--more-->
The routing structure only routes http requests. You can’t just redirect regular
socket connections.</p>

<p>WEBSOCKETS TO THE RESCUE!  Websockets are just sockets, but they are negotiated
over http. Oh yeah they are. So they can be routed by your infrastructure that
supports websockets. (like, say, <a href="http://github.com/substack/bouncy">bouncy</a>, but
there are plenty of other examples).</p>

<p>But you aren’t using a browser. There’s no browser here. Hmm.</p>

<p>You go check out <code>npm</code> and find out that maxogden has already solved your problem.
That’s what you do. He has already wrapped websockets with stream functionality in
a project he calls <a href="http://github.com/maxogden/websocket-stream">websocket-stream</a></p>

<p>Now your carefully designed code that leverages the node stream api can continue
on untouched simply by wrapping the websockets with this little gem.</p>

<p>Enjoy!</p>
]]></content>
  </entry>
  
</feed>
